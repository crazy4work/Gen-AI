{"cells":[{"cell_type":"code","execution_count":null,"id":"23d1f250","metadata":{"id":"23d1f250"},"outputs":[],"source":["from math import log2\n","import random\n","import numpy as np\n","import os\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torchvision.utils import save_image\n","import torchvision.datasets as datasets\n","import torchvision.transforms as transforms\n","import torch.optim as optim\n","from torch.utils.data import DataLoader\n","from tqdm import tqdm\n","import matplotlib.pyplot as plt"]},{"cell_type":"code","execution_count":null,"id":"ruVbSzPOy0hu","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":27280,"status":"ok","timestamp":1721691259555,"user":{"displayName":"Suparna Podder","userId":"15892563251693290892"},"user_tz":-330},"id":"ruVbSzPOy0hu","outputId":"bf6e51f9-63a7-4cca-eb5f-e2c45461b994"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"id":"9dc52225","metadata":{"id":"9dc52225"},"outputs":[],"source":["def seed_everything(seed=42):\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","seed_everything()"]},{"cell_type":"code","execution_count":null,"id":"35ee8437","metadata":{"id":"35ee8437"},"outputs":[],"source":["DATASETPATH                 = '/content/drive/MyDrive/Generative models/datasets/gray'\n","START_TRAIN_AT_IMG_SIZE = 4\n","DEVICE                  = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","CHECKPOINT_GEN          = \"generator.pth\"\n","CHECKPOINT_CRITIC       = \"critic.pth\"\n","SAVE_MODEL              = True\n","LOAD_MODEL              = False\n","LEARNING_RATE           = 1e-3\n","BATCH_SIZES             = [32, 32, 32, 16, 16, 16, 16, 8, 4]\n","image_size              = 1024\n","CHANNELS_IMG            = 1\n","Z_DIM                   = 512\n","IN_CHANNELS             = 512\n","CRITIC_ITERATIONS       = 1\n","LAMBDA_GP               = 10\n","PROGRESSIVE_EPOCHS      = [50] * len(BATCH_SIZES)\n","FIXED_NOISE             = torch.randn(9, Z_DIM, 1, 1).to(DEVICE)\n","NUM_WORKERS = 4"]},{"cell_type":"code","execution_count":null,"id":"22187db5","metadata":{"id":"22187db5"},"outputs":[],"source":["class WSConv2d(nn.Module):\n","    \"\"\"\n","    This is the wt scaling conv layer layer. Initialize with N(0, scale). Then it will multiply the scale for every forward pass\n","    \"\"\"\n","    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1, gain=np.sqrt(2)):\n","        super().__init__()\n","        self.conv = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=1, padding=padding)\n","\n","\n","        bias = self.conv.bias\n","        self.bias = nn.Parameter(bias.view(1, bias.shape[0], 1, 1))\n","        self.conv.bias = None\n","\n","\n","        convShape = list(self.conv.weight.shape)\n","        fanIn = np.prod(convShape[1:]) # Leave out # of o/p filters\n","        self.wtScale = gain/np.sqrt(fanIn)\n","\n","\n","        nn.init.normal_(self.conv.weight)\n","        nn.init.constant_(self.bias, val=0)\n","\n","\n","    def forward(self, x):\n","        #return self.conv(x)\n","        return self.conv(x * self.wtScale) + self.bias\n","\n","    def __repr__(self):\n","        convShape = list(self.conv.weight.shape)\n","        return f\"{self.__class__.__name__}(in_channels={convShape[1]}, out_channels={convShape[0]}, kernel_size={self.conv.kernel_size}, padding={self.conv.padding})\"\n"]},{"cell_type":"code","execution_count":null,"id":"f6711975","metadata":{"id":"f6711975"},"outputs":[],"source":["class WSLinear(nn.Module):\n","\n","    def __init__(self, in_dim, out_dim):\n","        super().__init__()\n","        self.linear = nn.Linear(in_dim, out_dim)\n","        self.bias = self.linear.bias\n","        self.linear.bias = None\n","        fanIn = in_dim\n","        self.wtScale = np.sqrt(2) / np.sqrt(fanIn)\n","\n","        nn.init.normal_(self.linear.weight)\n","        nn.init.constant_(self.bias, val=0)\n","\n","    def forward(self, x):\n","        x = x.view(x.shape[0], -1)\n","        return self.linear(x * self.wtScale) + self.bias"]},{"cell_type":"code","execution_count":null,"id":"d290555e","metadata":{"id":"d290555e"},"outputs":[],"source":["class PixelNorm(nn.Module):\n","\n","    def __init__(self):\n","        super().__init__()\n","\n","    def forward(self, x):\n","        print(\"PixelwiseNormalization\",x.shape)\n","        factor = ((x**2).mean(dim=1, keepdim=True) + 1e-8)**0.5\n","        return x / factor"]},{"cell_type":"code","execution_count":null,"id":"00842626","metadata":{"id":"00842626"},"outputs":[],"source":["class UpSamplingBlock(nn.Module):\n","    def __init__(self):\n","        super(UpSamplingBlock, self).__init__()\n","\n","    def forward(self, x):\n","        #bilinear interpolation\n","        return nn.functional.interpolate(x, scale_factor=2)"]},{"cell_type":"code","execution_count":null,"id":"56fc8b2e","metadata":{"id":"56fc8b2e"},"outputs":[],"source":["class MinibatchStdLayer(nn.Module):\n","\n","    def __init__(self, group_size=4):\n","        super().__init__()\n","        self.group_size = group_size\n","\n","    # Implementation from:\n","    # https://github.com/facebookresearch/pytorch_GAN_zoo/blob/master/models/networks/custom_layers.py\n","    def forward(self, x):\n","        size = x.size()\n","        #subGroupSize = min(size[0], self.group_size)\n","        subGroupSize=self.group_size\n","        '''if size[0] % subGroupSize != 0:\n","            subGroupSize = size[0]'''\n","        G = int(size[0] / subGroupSize)\n","        if subGroupSize > 1:\n","            y = x.view(-1, subGroupSize, size[1], size[2], size[3])\n","            y = torch.var(y, 1)\n","            y = torch.sqrt(y + 1e-8)\n","            y = y.view(G, -1)\n","            y = torch.mean(y, 1).view(G, 1)\n","            y = y.expand(G, size[2]*size[3]).view((G, 1, 1, size[2], size[3]))\n","            y = y.expand(G, subGroupSize, -1, -1, -1)\n","            y = y.contiguous().view((-1, 1, size[2], size[3]))\n","        else:\n","            y = torch.zeros(x.size(0), 1, x.size(2), x.size(3), device=x.device)\n","\n","        return torch.cat([x, y], dim=1)"]},{"cell_type":"code","execution_count":null,"id":"605143d8","metadata":{"id":"605143d8"},"outputs":[],"source":["factors = [1, 1, 1, 1, 1 / 2, 1 / 4, 1 / 8, 1 / 16, 1 / 32]"]},{"cell_type":"code","execution_count":null,"id":"fda6d3cd","metadata":{"id":"fda6d3cd"},"outputs":[],"source":["class ConvBlock(nn.Module):\n","    def __init__(self, in_channels, out_channels, use_pixelnorm=True):\n","        super(ConvBlock, self).__init__()\n","        self.use_pn = use_pixelnorm\n","        self.conv1 = WSConv2d(in_channels, out_channels)\n","        self.conv2 = WSConv2d(out_channels, out_channels)\n","        self.leaky = nn.LeakyReLU(0.2)\n","        self.pn = PixelNorm()\n","\n","    def forward(self, x):\n","        x = self.leaky(self.conv1(x))\n","        x = self.pn(x) if self.use_pn else x\n","        x = self.leaky(self.conv2(x))\n","        x = self.pn(x) if self.use_pn else x\n","        return x\n","\n","\n","class Generator(nn.Module):\n","    def __init__(self, z_dim, in_channels, img_channels=3):\n","        super(Generator, self).__init__()\n","\n","        self.initial = nn.Sequential(\n","            PixelNorm(),\n","            nn.ConvTranspose2d(z_dim, in_channels, 4, 1, 0),\n","            nn.LeakyReLU(0.2),\n","            WSConv2d(in_channels, in_channels, kernel_size=3, stride=1, padding=1),\n","            nn.LeakyReLU(0.2),\n","            PixelNorm(),\n","        )\n","\n","        self.initial_rgb = WSConv2d(\n","            in_channels, img_channels, kernel_size=1, stride=1, padding=0\n","        )\n","        self.prog_blocks, self.rgb_layers = (\n","            nn.ModuleList([]),\n","            nn.ModuleList([self.initial_rgb]),\n","        )\n","\n","        for i in range(\n","            len(factors) - 1\n","        ):\n","            conv_in_c = int(in_channels * factors[i])\n","            conv_out_c = int(in_channels * factors[i + 1])\n","            self.prog_blocks.append(ConvBlock(conv_in_c, conv_out_c))\n","            self.rgb_layers.append(\n","                WSConv2d(conv_out_c, img_channels, kernel_size=1, stride=1, padding=0)\n","            )\n","\n","    def fade_in(self, alpha, upscaled, generated):\n","        return torch.tanh(alpha * generated + (1 - alpha) * upscaled)\n","\n","    def forward(self, x, alpha, steps):\n","        out = self.initial(x)\n","\n","        if steps == 0:\n","            return self.initial_rgb(out)\n","\n","        for step in range(steps):\n","            upscaled = F.interpolate(out, scale_factor=2)\n","            out = self.prog_blocks[step](upscaled)\n","\n","        final_upscaled = self.rgb_layers[steps - 1](upscaled)\n","        final_out = self.rgb_layers[steps](out)\n","        return self.fade_in(alpha, final_upscaled, final_out)\n","\n","\n","class Discriminator(nn.Module):\n","    def __init__(self, z_dim, in_channels, img_channels=3):\n","        super(Discriminator, self).__init__()\n","        self.prog_blocks, self.rgb_layers = nn.ModuleList([]), nn.ModuleList([])\n","        self.leaky = nn.LeakyReLU(0.2)\n","\n","\n","        for i in range(len(factors) - 1, 0, -1):\n","            conv_in = int(in_channels * factors[i])\n","            conv_out = int(in_channels * factors[i - 1])\n","            self.prog_blocks.append(ConvBlock(conv_in, conv_out, use_pixelnorm=False))\n","            self.rgb_layers.append(\n","                WSConv2d(img_channels, conv_in, kernel_size=1, stride=1, padding=0)\n","            )\n","\n","        self.initial_rgb = WSConv2d(\n","            img_channels, in_channels, kernel_size=1, stride=1, padding=0\n","        )\n","        self.rgb_layers.append(self.initial_rgb)\n","        self.avg_pool = nn.AvgPool2d(\n","            kernel_size=2, stride=2\n","        )\n","        self.final_block = nn.Sequential(\n","            WSConv2d(in_channels + 1, in_channels, kernel_size=3, padding=1),\n","            nn.LeakyReLU(0.2),\n","            WSConv2d(in_channels, in_channels, kernel_size=4, padding=0, stride=1),\n","            nn.LeakyReLU(0.2),\n","            WSLinear(in_channels, 1),\n","        )\n","\n","    def fade_in(self, alpha, downscaled, out):\n","\n","        return alpha * out + (1 - alpha) * downscaled\n","\n","    def minibatch_std(self, x):\n","        batch_statistics = (\n","            torch.std(x, dim=0).mean().repeat(x.shape[0], 1, x.shape[2], x.shape[3])\n","        )\n","\n","        return torch.cat([x, batch_statistics], dim=1)\n","\n","    def forward(self, x, alpha, steps):\n","\n","        cur_step = len(self.prog_blocks) - steps\n","\n","\n","        out = self.leaky(self.rgb_layers[cur_step](x))\n","\n","        if steps == 0:\n","            out = self.minibatch_std(out)\n","            return self.final_block(out).view(out.shape[0], -1)\n","\n","\n","        downscaled = self.leaky(self.rgb_layers[cur_step + 1](self.avg_pool(x)))\n","        out = self.avg_pool(self.prog_blocks[cur_step](out))\n","\n","\n","        out = self.fade_in(alpha, downscaled, out)\n","\n","        for step in range(cur_step + 1, len(self.prog_blocks)):\n","            out = self.prog_blocks[step](out)\n","            out = self.avg_pool(out)\n","\n","        out = self.minibatch_std(out)\n","        return self.final_block(out).view(out.shape[0], -1)\n"]},{"cell_type":"code","execution_count":null,"id":"e22a940e","metadata":{"id":"e22a940e"},"outputs":[],"source":["#testing ip op shapes\n","if __name__==\"__main__\":\n","    Z_DIM=512\n","    IN_CHANNELS=512\n","    gen=Generator(Z_DIM,IN_CHANNELS,img_channels=1)\n","    critic=Discriminator(Z_DIM,IN_CHANNELS, img_channels=1)\n","\n","    for img_size in [4,8,16,32,64,128,256,512,1024]:\n","        num_steps=int(log2(img_size/4))\n","        x=torch.randn((1, Z_DIM, 1, 1))\n","        z=gen(x, 0.5, steps=num_steps)\n","        assert z.shape== (1, 1, img_size, img_size)\n","        out=critic(z, 0.5, steps=num_steps)\n","        assert out.shape==(1,1)\n","        print(f\"success at img size: {img_size}\")"]},{"cell_type":"markdown","id":"7f4b9ae0","metadata":{"id":"7f4b9ae0"},"source":["#utils"]},{"cell_type":"code","execution_count":null,"id":"29f343e2","metadata":{"id":"29f343e2"},"outputs":[],"source":["import torch\n","import random\n","import numpy as np\n","import os\n","import torchvision\n","import torch.nn as nn\n","#import config\n","from torchvision.utils import save_image\n","from scipy.stats import truncnorm\n","\n","# Print losses occasionally and print to tensorboard\n","def plot_to_tensorboard(\n","    writer, loss_critic, loss_gen, real, fake, tensorboard_step\n","):\n","    writer.add_scalar(\"Loss Critic\", loss_critic, global_step=tensorboard_step)\n","\n","    with torch.no_grad():\n","        # take out (up to) 8 examples to plot\n","        img_grid_real = torchvision.utils.make_grid(real[:8], normalize=True)\n","        img_grid_fake = torchvision.utils.make_grid(fake[:8], normalize=True)\n","        writer.add_image(\"Real\", img_grid_real, global_step=tensorboard_step)\n","        writer.add_image(\"Fake\", img_grid_fake, global_step=tensorboard_step)\n","\n","\n","def gradient_penalty(critic, real, fake, alpha, train_step, device=\"cpu\"):\n","    BATCH_SIZE, C, H, W = real.shape\n","    beta = torch.rand((BATCH_SIZE, 1, 1, 1)).repeat(1, C, H, W).to(device)\n","    interpolated_images = real * beta + fake.detach() * (1 - beta)\n","    interpolated_images.requires_grad_(True)\n","\n","    # Calculate critic scores\n","    mixed_scores = critic(interpolated_images, alpha, train_step)\n","\n","    # Take the gradient of the scores with respect to the images\n","    gradient = torch.autograd.grad(\n","        inputs=interpolated_images,\n","        outputs=mixed_scores,\n","        grad_outputs=torch.ones_like(mixed_scores),\n","        create_graph=True,\n","        retain_graph=True,\n","    )[0]\n","    gradient = gradient.view(gradient.shape[0], -1)\n","    gradient_norm = gradient.norm(2, dim=1)\n","    gradient_penalty = torch.mean((gradient_norm - 1) ** 2)\n","    return gradient_penalty\n","\n","\n","def save_checkpoint(model, optimizer, filename=\"my_checkpoint.pth.tar\"):\n","    print(\"=> Saving checkpoint\")\n","    checkpoint = {\n","        \"state_dict\": model.state_dict(),\n","        \"optimizer\": optimizer.state_dict(),\n","    }\n","    torch.save(checkpoint, filename)\n","\n","\n","def load_checkpoint(checkpoint_file, model, optimizer, lr):\n","    print(\"=> Loading checkpoint\")\n","    checkpoint = torch.load(checkpoint_file, map_location=\"cuda\")\n","    model.load_state_dict(checkpoint[\"state_dict\"])\n","    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n","\n","    # If we don't do this then it will just have learning rate of old checkpoint\n","    # and it will lead to many hours of debugging \\:\n","    for param_group in optimizer.param_groups:\n","        param_group[\"lr\"] = lr\n","\n","def seed_everything(seed=42):\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","\n","def generate_examples(gen, steps, truncation=0.7, n=100):\n","    \"\"\"\n","    Tried using truncation trick here but not sure it actually helped anything, you can\n","    remove it if you like and just sample from torch.randn\n","    \"\"\"\n","    gen.eval()\n","    alpha = 1.0\n","    for i in range(n):\n","        with torch.no_grad():\n","            noise = torch.tensor(truncnorm.rvs(-truncation, truncation, size=(1, Z_DIM, 1, 1)), device=DEVICE, dtype=torch.float32)\n","            img = gen(noise, alpha, steps)\n","            save_image(img*0.5+0.5, f\"saved_examples/img_{i}.png\")\n","    gen.train()"]},{"cell_type":"code","execution_count":null,"id":"9128de6c","metadata":{"id":"9128de6c"},"outputs":[],"source":["def train_fn(\n","    critic,\n","    gen,\n","    loader,\n","    dataset,\n","    step,\n","    alpha,\n","    opt_critic,\n","    opt_gen,\n","):\n","    loop = tqdm(loader, leave=True)\n","    for batch_idx, (real, _) in enumerate(loop):\n","        real = real.to(DEVICE)\n","        cur_batch_size = real.shape[0]\n","\n","        noise = torch.randn(cur_batch_size, Z_DIM, 1, 1).to(DEVICE)\n","\n","        fake = gen(noise, alpha, step)\n","        critic_real = critic(real, alpha, step)\n","        critic_fake = critic(fake.detach(), alpha, step)\n","        gp = gradient_penalty(critic, real, fake, alpha, step, device=DEVICE)\n","        loss_critic = (\n","            -(torch.mean(critic_real) - torch.mean(critic_fake))\n","            + LAMBDA_GP * gp\n","            + (0.001 * torch.mean(critic_real ** 2))\n","        )\n","\n","        critic.zero_grad()\n","        loss_critic.backward()\n","        opt_critic.step()\n","\n","        gen_fake = critic(fake, alpha, step)\n","        loss_gen = -torch.mean(gen_fake)\n","\n","        gen.zero_grad()\n","        loss_gen.backward()\n","        opt_gen.step()\n","\n","        alpha += cur_batch_size / (\n","            (PROGRESSIVE_EPOCHS[step] * 0.5) * len(dataset)\n","        )\n","        alpha = min(alpha, 1)\n","\n","        loop.set_postfix(\n","            gp=gp.item(),\n","            loss_critic=loss_critic.item(),\n","        )\n","\n","\n","    return alpha"]},{"cell_type":"code","execution_count":null,"id":"e1eed60e","metadata":{"id":"e1eed60e"},"outputs":[],"source":["from PIL import Image\n","from torch.utils.data import DataLoader, Dataset\n","import os\n","class CustomDataset(Dataset):\n","    def __init__(self, root, count=5181, transform=None):\n","        self.root = root\n","        self.images = []\n","\n","        # Load the images from the dataset\n","        for image_path in os.listdir(root):\n","            image = Image.open(os.path.join(root, image_path))\n","            if transform is not None:\n","                image = transform(image)\n","            self.images.append(image)\n","            #count-=1\n","            #if(count==0):\n","             # break\n","\n","    def __len__(self):\n","        return len(self.images)\n","\n","    def __getitem__(self, idx):\n","        return self.images[idx]"]},{"cell_type":"code","execution_count":null,"id":"63a5666c","metadata":{"id":"63a5666c"},"outputs":[],"source":["import os\n","from PIL import Image\n","#input_dir='/content/drive/MyDrive/Generative models/datasets/cc'\n","input_dir='/content/drive/MyDrive/Generative models/datasets/mlo'\n","output_dir='/content/drive/MyDrive/Generative models/datasets/gray'\n","os.makedirs(output_dir, exist_ok=True)\n","\n","# Iterate over all files in the directory\n","for filename in os.listdir(input_dir):\n","    if filename.endswith('.png'):  # Assuming all images have .png extension\n","        # Open the image\n","        img = Image.open(os.path.join(input_dir, filename))\n","\n","        # Convert the image to grayscale if it's not already grayscale\n","        if img.mode != 'L':\n","            img = img.convert('L')\n","\n","        output_path = os.path.join(output_dir, filename)\n","        img.save(output_path)"]},{"cell_type":"code","execution_count":null,"id":"ccac1fc1","metadata":{"id":"ccac1fc1"},"outputs":[],"source":["def get_loader(image_size):\n","    transform = transforms.Compose(\n","        [\n","            transforms.Resize((image_size, image_size)),\n","            transforms.ToTensor(),\n","            transforms.RandomHorizontalFlip(p=0.5),\n","            transforms.Normalize(\n","                [0.5 for _ in range(CHANNELS_IMG)],\n","                [0.5 for _ in range(CHANNELS_IMG)],\n","            ),\n","        ]\n","    )\n","    batch_size = BATCH_SIZES[int(log2(image_size / 4))]\n","    #dataset = datasets.ImageFolder(root=DATASET, transform=transform)\n","    #data_path='../datasets/gray'\n","    dataset=CustomDataset(DATASETPATH,  transform=transform)\n","    loader = DataLoader(\n","        dataset,\n","        batch_size=batch_size,\n","        shuffle=True,\n","        num_workers=NUM_WORKERS,\n","        pin_memory=True,\n","    )\n","    return loader, dataset"]},{"cell_type":"code","execution_count":null,"id":"7f1d9e37","metadata":{"id":"7f1d9e37"},"outputs":[],"source":["def check_loader():\n","    loader,_ = get_loader(1024)\n","    cloth  = next(iter(loader))\n","    _, ax = plt.subplots(2,3, figsize=(8,5))\n","    plt.suptitle('Some real samples', fontsize=15, fontweight='bold')\n","    ind = 0\n","    for k in range(2):\n","        for kk in range(3):\n","\n","            ax[k][kk].imshow((cloth[ind].permute(1,2,0)+1)/2)\n","            ind += 1\n","    '''def show_images(images):\n","        fig, ax = plt.subplots(figsize=(8, 8))\n","        ax.set_xticks([]); ax.set_yticks([])\n","        ax.imshow(make_grid(images.cpu().detach()[:64], nrow=8).permute(1, 2, 0))\n","\n","\n","    for images in loader:\n","        show_images(images)\n","        break'''\n","check_loader()"]},{"cell_type":"markdown","id":"ff3f08a2","metadata":{"id":"ff3f08a2"},"source":["Train.py"]},{"cell_type":"code","execution_count":null,"id":"7b158c4d","metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","output_embedded_package_id":"1owG9SWrXievrmCBil1cM1ydfXkAAgJK3"},"id":"7b158c4d","outputId":"77d9c95f-eb36-442c-9363-2699ae3f692c"},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["\"\"\" Training of ProGAN using WGAN-GP loss\"\"\"\n","\n","import torch\n","import torch.optim as optim\n","import torchvision.datasets as datasets\n","import torchvision.transforms as transforms\n","from torch.utils.data import DataLoader\n","from torch.utils.tensorboard import SummaryWriter\n","'''from utils import (\n","    gradient_penalty,\n","    plot_to_tensorboard,\n","    save_checkpoint,\n","    load_checkpoint,\n","    generate_examples,\n",")'''\n","#from model import Discriminator, Generator\n","from math import log2\n","from tqdm import tqdm\n","#import config\n","\n","torch.backends.cudnn.benchmarks = True\n","\n","\n","\n","\n","\n","def train_fn(\n","    critic,\n","    gen,\n","    loader,\n","    dataset,\n","    step,\n","    alpha,\n","    opt_critic,\n","    opt_gen,\n","    tensorboard_step,\n","    writer,\n","    scaler_gen,\n","    scaler_critic,\n","):\n","    loop = tqdm(loader, leave=True)\n","    for batch_idx, real in enumerate(loop):\n","        real = real.to(DEVICE)\n","        cur_batch_size = real.shape[0]\n","\n","        # Train Critic: max E[critic(real)] - E[critic(fake)] <-> min -E[critic(real)] + E[critic(fake)]\n","        # which is equivalent to minimizing the negative of the expression\n","        noise = torch.randn(cur_batch_size, Z_DIM, 1, 1).to(DEVICE)\n","\n","        with torch.cuda.amp.autocast():\n","            fake = gen(noise, alpha, step)\n","            critic_real = critic(real, alpha, step)\n","            critic_fake = critic(fake.detach(), alpha, step)\n","            gp = gradient_penalty(critic, real, fake, alpha, step, device=DEVICE)\n","            loss_critic = (\n","                -(torch.mean(critic_real) - torch.mean(critic_fake))\n","                + LAMBDA_GP * gp\n","                + (0.001 * torch.mean(critic_real ** 2))\n","            )\n","\n","        opt_critic.zero_grad()\n","        scaler_critic.scale(loss_critic).backward()\n","        scaler_critic.step(opt_critic)\n","        scaler_critic.update()\n","\n","        # Train Generator: max E[critic(gen_fake)] <-> min -E[critic(gen_fake)]\n","        with torch.cuda.amp.autocast():\n","            gen_fake = critic(fake, alpha, step)\n","            loss_gen = -torch.mean(gen_fake)\n","\n","        opt_gen.zero_grad()\n","        scaler_gen.scale(loss_gen).backward()\n","        scaler_gen.step(opt_gen)\n","        scaler_gen.update()\n","\n","        # Update alpha and ensure less than 1\n","        alpha += cur_batch_size / (\n","            (PROGRESSIVE_EPOCHS[step] * 0.5) * len(dataset)\n","        )\n","        alpha = min(alpha, 1)\n","\n","        if batch_idx % 500 == 0:\n","            with torch.no_grad():\n","                fixed_fakes = gen(FIXED_NOISE, alpha, step) * 0.5 + 0.5\n","            plot_to_tensorboard(\n","                writer,\n","                loss_critic.item(),\n","                loss_gen.item(),\n","                real.detach(),\n","                fixed_fakes.detach(),\n","                tensorboard_step,\n","            )\n","            tensorboard_step += 1\n","\n","        loop.set_postfix(\n","            gp=gp.item(),\n","            loss_critic=loss_critic.item(),\n","        )\n","\n","    return tensorboard_step, alpha\n","\n","\n","def main():\n","    # initialize gen and disc, note: discriminator should be called critic,\n","    # according to WGAN paper (since it no longer outputs between [0, 1])\n","    # but really who cares..\n","    gen = Generator(Z_DIM, IN_CHANNELS, img_channels=CHANNELS_IMG).to(DEVICE)\n","    critic = Discriminator(Z_DIM, IN_CHANNELS, img_channels=CHANNELS_IMG).to(DEVICE)\n","\n","    # initialize optimizers and scalers for FP16 training\n","    opt_gen = optim.Adam(gen.parameters(), lr=LEARNING_RATE, betas=(0.0, 0.99))\n","    opt_critic = optim.Adam(critic.parameters(), lr=LEARNING_RATE, betas=(0.0, 0.99))\n","    scaler_critic = torch.cuda.amp.GradScaler()\n","    scaler_gen = torch.cuda.amp.GradScaler()\n","\n","    # for tensorboard plotting\n","    writer = SummaryWriter(f\"logs/gan1\")\n","\n","    if LOAD_MODEL:\n","        load_checkpoint(CHECKPOINT_GEN, gen, opt_gen, LEARNING_RATE,)\n","        load_checkpoint(CHECKPOINT_CRITIC, critic, opt_critic, LEARNING_RATE,)\n","\n","    gen.train()\n","    critic.train()\n","\n","    tensorboard_step = 0\n","    # start at step that corresponds to img size that we set in config\n","    step = int(log2(START_TRAIN_AT_IMG_SIZE / 4))\n","    for num_epochs in PROGRESSIVE_EPOCHS[step:]:\n","        alpha = 1e-5  # start with very low alpha\n","        loader, dataset = get_loader(4 * 2 ** step)  # 4->0, 8->1, 16->2, 32->3, 64 -> 4\n","        print(f\"Current image size: {4 * 2 ** step}\")\n","\n","        for epoch in range(num_epochs):\n","            print(f\"Epoch [{epoch+1}/{num_epochs}]\")\n","            tensorboard_step, alpha = train_fn(\n","                critic,\n","                gen,\n","                loader,\n","                dataset,\n","                step,\n","                alpha,\n","                opt_critic,\n","                opt_gen,\n","                tensorboard_step,\n","                writer,\n","                scaler_gen,\n","                scaler_critic,\n","            )\n","\n","            if SAVE_MODEL:\n","                save_checkpoint(gen, opt_gen, filename=CHECKPOINT_GEN)\n","                save_checkpoint(critic, opt_critic, filename=CHECKPOINT_CRITIC)\n","\n","        step += 1  # progress to the next img size\n","\n","\n","if __name__ == \"__main__\":\n","    main()"]},{"cell_type":"code","execution_count":null,"id":"65fdafab","metadata":{"id":"65fdafab"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"}},"nbformat":4,"nbformat_minor":5}